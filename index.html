<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Divya Kothandaraman</title>

  <meta name="author" content="Divya Kothandaraman">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/x-icon" href="images/umd_logo.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Divya Kothandaraman</name>
                  </p>
                  <p>I am a senior researcher at Dolby Laboratories. I earned my PhD in Computer Science from the University of 
                    Maryland, College Park, my dissertation, "Learning from Less Data: Perception and Synthesis," explores 
                    data-efficient AI. Prior to this, I was an undergraduate at the <a href="https://www.iitm.ac.in">Indian Institute of
                      Technology Madras</a>, where
                    I obtained a bachelors degree in Electrical Engineering, and masters degree in Data Sciences. 
                  </p>
                  <p>
                    My research interests encompass the intersection of generative AI, computer vision, and multi-modal learning. 
                    My goal is to advance computer vision, generative, and multimodal AI systems by developing 
                    methods grounded in core principles that are both foundational and application‑driven. I seek to 
                    contribute to solving diverse and challenging problems, enabling real‑world impact.
                  </p>
                  
                  <p style="text-align:center">
                    <a href="mailto:ramandivya27@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=ejLpAX0AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://twitter.com/DivyaKRaman1">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/divyakraman">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:100%;max-width:100%" alt="profile photo" src="images/dk.jpeg"
                    class="hoverZoomLink">
                </td>
              </tr>
            </tbody>
          </table>
        
          
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <font color="red">Latest News</font>
                  </heading>
                  <!-- <p align="left">
                    <font color="red"><strong>News:</strong></font>
                  </p> -->
                  <ul>
                    <li><b><span class="tab">(Aug 2025)</span></b> Concept blending paper has been accepted to ACMMM 2025 as an Oral! </li>
                    <li><b><span class="tab">(Aug 2025)</span></b> Presented HawkI at MIPR 2025 as an Oral! </li>
                    <li><b><span class="tab">(Jan 2025)</span></b> Joined Dolby Laboratories as a Senior Researcher! </li>
                    <li><b><span class="tab">(Nov 2024)</span></b> ImPoster has been accepted to COLING 2025! </li>
                    <li><b><span class="tab">(Nov 2024)</span></b> Defended my PhD! </li> 
                    <li><b><span class="tab">(Sep 2024)</span></b> Gave a talk on novel view synthesis at the ECCV 2024 Wild3D Workshop! </li> 
                    <li><b><span class="tab">(Mar 2024)</span></b> Gave a talk at UCL! Slides <a href="https://docs.google.com/presentation/d/1_vr6bqS8TGIQm4Sh8T5Nh2jHdA8ZXqJC/edit?usp=sharing&ouid=114075554212673783266&rtpof=true&sd=true">here. </a></li>
                    <li><b><span class="tab">(Sep 2023)</span></b> Aerial Diffusion has been accepted to Siggraph Asia 2023. </li> 
                    <li><b><span class="tab">(May 2023)</span></b> Interning at Google DeepMind. </li> 
                    <li><b><span class="tab">(Jan 2023)</span></b> Differentiable FAR has been accepted to ICRA 2023. </li> 
                    <li><b><span class="tab">(Oct 2022)</span></b> Two papers have been accepted to WACV 2023. </li> 
                    <li><b><span class="tab">(July 2022)</span></b> FAR: Fourier Aerial Video Recognition has been accepted to ECCV 2022. </li> 
                    
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
            <tr>
              <td style="padding:20px;width:100%">
                <heading>
                  <font color="red"> Selected Research (see Google Scholar for full list)</font>
                </heading>
              </td>
            </tr>
          </tbody>
        </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/promptmixing2024.jpg" alt="PromptMixing" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2405.13685">
                    <papertitle>Financial Models in Generative Art: Black-Scholes-Inspired Concept Blending in Text-to-Image Diffusion</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Ming Lin, Dinesh Manocha
                  <br>
                  <em>ACMMM 2025 (Oral)</em>
                  <br>
                   <a href="https://arxiv.org/abs/2405.13685">arXiv</a>                  
                   / <a
                    href="https://github.com/divyakraman/BlackScholesDiffusion2024">GitHub</a>
              <p>
              An approach for concept blending using novel perspectives from the Black Scholes model in economics and finance.  </p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/aerialbooth2023.jpg" alt="Aerial_Booth" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2311.15478">
                    <papertitle>HawkI: Homography and Mutual Information Guidance for 3D-free Single Image to Aerial View</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Tianyi Zhou, Ming Lin, Dinesh Manocha
                  <br>
                  <em>MIPR 2025 (Oral)</em>
                  <br>
                   <a href="https://arxiv.org/abs/2311.15478">arXiv</a>                  
                   / <a
                    href="https://github.com/divyakraman/AerialBooth2023">GitHub</a>
              <p>
              Mutual information and inverse perspective mapping guidance for text-controlled aerial view synthesis from a single input image using diffusion models.  </p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/imposter2024.jpg" alt="ImPoster" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2409.15650">
                    <papertitle>ImPoster: Text and Frequency Guidance for Subject Driven Action Personalization using Diffusion Models</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Kuldeep Kulkarni, Sumit Shekhar, Balaji Vasan Srinivasan, Dinesh Manocha
                  <br>
                  <em>COLING 2025</em>
                  <br>
                   <a href="https://arxiv.org/abs/2409.15650">arXiv</a>                  
                   / <a
                    href="https://github.com/divyakraman/ImPosterDiffusion2024">GitHub</a>
              <p>
              An approach for subject and action personalization using prompting techniques and concepts from image and signal processing.  </p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/MultiConceptVideo2024.jpg" alt="MultiConceptVideo" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2405.13951">
                    <papertitle>Text Prompting for Multi-Concept Video Customization by Autoregressive Generation</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Kihyuk Sohn, Ruben Villegas, Paul Voigtlaender, Dinesh Manocha, Mohammad Babaeizadeh
                  <br>
                  <em><a href="https://ai4cc.net/">AI4CC Workshop</a> at CVPR 2024</em>
                  <br>
                   <a href="https://arxiv.org/abs/2405.13951">arXiv</a>                  
              <p>
              Sequential and controlled autoregressive generation of the desired custom concepts for multi-concept customized video generation with transfoermer models. </p>
                </td>
              </tr>
       
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/aerialdiffusion2023.png" alt="Aerial_Diffusion" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2303.11444">
                    <papertitle>Aerial Diffusion: Text Guided Ground-to-Aerial View Translation from a Single Image using Diffusion Models</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Tianyi Zhou, Ming Lin, Dinesh Manocha
                  <br>
                  <em>Siggraph Asia 2023 (Conference Proceedings, Technical Communications)</em>
                  <br>
                   <a href="https://arxiv.org/abs/2303.11444">arXiv</a>                  
                   / <a
                    href="https://github.com/divyakraman/AerialDiffusion">GitHub</a>
              <p>
              A text-guided image to image diffusion model to generate aerial views from a single ground-view image.  </p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/differentiableFAR_2022.png" alt="DifFAR" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2209.09194.pdf">
                    <papertitle>Differentiable Frequency-based Disentanglement for Aerial Video Action Recognition
                    </papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Ming Lin, Dinesh Manocha
                  <br>
                  <em>ICRA 2023</em>
                  <br>
                  <a href="https://arxiv.org/pdf/2209.09194.pdf">arXiv</a>                  
                   / <a
                    href="https://github.com/divyakraman/DIFFAR2022_DifferentiableFrequencyBasedDisentanglement">GitHub</a>
              <p>
              A differentiable feature disentanglement method to learn "static salient" and "dynamic salient" regions for aerial video action recognition.  </p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/DistillAdapt_salad2022.png" alt="SALAD" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2205.12840">
                    <papertitle>SALAD: Source-free Active Label Agnostic Domain Adaptation</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Sumit Shekhar, Abhilasha Sancheti, Manoj Ghuhan, Tripti Shukla, Dinesh Manocha
                  <br>
                  <em>WACV 2023</em>
                  <br>
                  <!-- <a href="https://gamma.umd.edu/researchdirections/aerialvideos/far/">Project Page /</a> -->
                   <a href="https://arxiv.org/abs/2205.12840">arXiv</a>
                  <!-- / <a href="https://www.youtube.com/watch?v=rSPIah0liTA">YouTube</a> -->
                  / <a
                    href="https://github.com/divyakraman/SALAD_SourcefreeActiveLabelAgnosticDomainAdaptation">GitHub</a>
                  <!-- / <a href="data/far_eccv_2022.bib">bibtex</a> -->
              <p>
              A generic source-free active domain adaptation method that can handle shifts in output label space.  </p>
                </td>
              </tr>


            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/far_eccv22.png" alt="FAR" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2203.10694">
                    <papertitle>FAR: Fourier Aerial Video Recognition</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  Tianrui Guan, Xijun Wang, Sean Hu, Ming Lin, Dinesh Manocha
                  <br>
                  <em>ECCV 2022</em>
                  <br>
                  
                  <a href="https://gamma.umd.edu/researchdirections/aerialvideos/far/">Project Page</a>
                  / <a href="https://arxiv.org/abs/2203.10694">arXiv</a>
                  <!-- / <a href="https://www.youtube.com/watch?v=rSPIah0liTA">YouTube</a> -->
                  / <a
                    href="https://github.com/divyakraman/ECCV2022_FARFourierAerialVideoRecognition">GitHub</a>
                  <!-- / <a href="data/far_eccv_2022.bib">bibtex</a> -->
                
                  <p> An efficient aerial video action recognition method, with novel frequency domain techniques, vis-a-vis, Fourier object disentanglement and Fourier attention. 
              </p>
              
                </td>
              </tr>

      
      <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/ganav.png" alt="GANAV" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2103.04233">
                    <papertitle>GANav: Group-wise Attention Network for Classifying Navigable Regions in Unstructured Outdoor Environments</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?hl=en&user=_7mX21UAAAAJ"> Tianrui Guan </a>
                  <strong>Divya Kothandaraman</strong>,
                  <a href="https://rohanchandra30.github.io/">Rohan Chandra</a>
                  <a href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
                  <br>
                  <em>IROS 2022 and RSS 2022</em>
                  <br>
                  <a href="https://gamma.umd.edu/researchdirections/autonomousdriving/offroad/">Project Page</a>
                  / <a href="https://arxiv.org/abs/2103.04233">arXiv</a>
                  / <a href="data/ganav.bib">bibtex</a>
                  <p>An attention-based segmentation method for identifying safe and navigable regions in off-road terrains. </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/sssfda.png" alt="SS-SFDA" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2012.08939">
                    <papertitle>SS-SFDA : Self-Supervised Source-Free Domain Adaptation for Road Segmentation in Hazardous Environments</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  <a href="https://rohanchandra30.github.io/">Rohan Chandra</a>
                  <a href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
                  <br>
                  <em>ICCV Workshops 2021</em>
                  <br>
                  <a href="https://gamma.umd.edu/researchdirections/autonomousdriving/weathersafe/">Project Page</a>
                  / <a href="https://arxiv.org/abs/2012.08939">arXiv</a>
                  / <a href="https://www.youtube.com/watch?v=rSPIah0liTA">YouTube</a>
                  / <a
                    href="https://github.com/divyakraman/SS-SFDA-Self-Supervised-Source-Free-Domain-Adaptation-for-Road-Segmentation-in-Hazardous-Environme">GitHub</a>
                  / <a href="data/sssfda.bib">bibtex</a>
                  <p>A self-supervised learning approach for source free unsupervised road segmentation in adverse weather environments and low light conditions. </p>
                </td>
              </tr>

              
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/bomuda.png" alt="BOMUDA" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2010.03523">
                    <papertitle>BoMuDA: Boundless Multi-Source Domain Adaptive Segmentation in Unconstrained Environments</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  <a href="https://rohanchandra30.github.io/">Rohan Chandra</a>
                  <a href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
                  <br>
                  <em>ICCV Workshops 2021</em>
                  <br>
                  <a href="https://gamma.umd.edu/researchdirections/autonomousdriving/bomuda/">Project Page</a>
                  / <a href="https://arxiv.org/abs/2010.03523">arXiv</a>
                  / <a href="https://www.youtube.com/watch?v=nTusLNHRbV0">YouTube</a>
                  / <a
                    href="https://github.com/divyakraman/BoMuDA-Boundless-Multi-Source-Domain-Adaptive-Segmentation-in-Unstructured-Environments">GitHub</a>
                  / <a href="data/sssfda.bib">bibtex</a>
                  <p>A multi-source boundless unsupervised domain adaptation algorithm for semantic segmentation in unstructured environments. </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/wacv21iitm.png" alt="WACV21IITM" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Kothandaraman_Domain_Adaptive_Knowledge_Distillation_for_Driving_Scene_Semantic_Segmentation_WACVW_2021_paper.pdf">
                    <papertitle>Domain Adaptive Knowledge Distillation for Driving Scene Semantic Segmentation</papertitle>
                  </a>
                  <br>
                  <strong>Divya Kothandaraman</strong>,
                  <a href="https://scholar.google.com/citations?user=On_rfbcAAAAJ&hl=en&oi=ao">Athira Nambiar</a>
                  <a href="https://scholar.google.com/citations?hl=en&user=mB9AZSAAAAAJ">Anurag Mittal</a>
                  <br>
                  <em>WACV Workshops 2021</em>
                  <br>
                  <a href="https://openaccess.thecvf.com/content/WACV2021W/AVV/papers/Kothandaraman_Domain_Adaptive_Knowledge_Distillation_for_Driving_Scene_Semantic_Segmentation_WACVW_2021_paper.pdf">Paper</a>
                  / <a href="https://www.youtube.com/watch?v=1VwUFNs7WQA">YouTube</a>
                  / <a
                    href="https://github.com/divyakraman/Domain-Adaptive-Knowledge-Distillation-for-Driving-Scene-Semantic-Segmentation">GitHub</a>
                  / <a href="data/wacv21iitm.bib">bibtex</a>
                  <p>An approach for domain adaptive semantic segmentation in models with limited memory.</p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/eccvw20iitm.png" alt="ECCVW20" style="border-style: none" width="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2008.06229">
                    <papertitle>Deep Atrous Guided Filter for Image Restoration in Under Display Cameras</papertitle>
                  </a>
                  <br>
                  <a href="https://varun19299.github.io/">Varun Sundar </a>,
                    <a href="https://github.com/SumanthRH">Sumanth Hegde*</a>,
                    <strong>Divya Kothandaraman </strong>,
                    <a href="http://www.ee.iitm.ac.in/kmitra/">Kaushik Mitra</a><br>
                    <em>ECCV Workshops, 2020</em>
                    <br>
                  <a href="https://arxiv.org/abs/2008.06229">ArXiv</a>
                  / <a href="https://www.youtube.com/watch?v=WnNOg178iSk">YouTube</a>
                  / <a href="https://varun19299.github.io/deep-atrous-guided-filter/"> Project Page </a>
                  / <a href="data/eccvwiitm.bib">bibtex</a>
                  <p>Guided Filters when incorporated in a deep network can efficiently recover severely degraded, mega-pixel resolution images. </p>
                </td>
              </tr>


            </tbody>
          </table>



        </td>
      </tr>
    </tbody>
  </table>

  <table width="80%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="center">
          <font size="2">
            <a href="https://jonbarron.info/">Website template borrowed from Jon
              Barron</a>.</a>
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
